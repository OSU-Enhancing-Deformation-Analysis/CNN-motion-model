{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from einops import rearrange\n",
    "from torch import einsum\n",
    "import numpy as np\n",
    "from typing import cast\n",
    "\n",
    "# --------------------\n",
    "# Part Classes\n",
    "# --------------------\n",
    "\n",
    "class Aggregate(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        args,\n",
    "        dim,\n",
    "        heads=4,\n",
    "        dim_head=128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head**-0.5\n",
    "        inner_dim = heads * dim_head\n",
    "\n",
    "        self.to_v = nn.Conv2d(dim, inner_dim, 1, bias=False)\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        if dim != inner_dim:\n",
    "            self.project = nn.Conv2d(inner_dim, dim, 1, bias=False)\n",
    "        else:\n",
    "            self.project = None\n",
    "\n",
    "    def forward(self, attn, fmap):\n",
    "        heads, b, c, h, w = self.heads, *fmap.shape\n",
    "\n",
    "        v = self.to_v(fmap)\n",
    "        v = rearrange(v, \"b (h d) x y -> b h (x y) d\", h=heads)\n",
    "        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "\n",
    "        if self.project is not None:\n",
    "            out = self.project(out)\n",
    "\n",
    "        out = fmap + self.gamma * out\n",
    "\n",
    "        return out\n",
    "\n",
    "class twins_svt_large(nn.Module):\n",
    "    def __init__(self, pretrained=True, del_layers=True, in_chans=3): # Added in_chans\n",
    "        super().__init__()\n",
    "        self.svt = timm.create_model(\"twins_svt_large\", pretrained=pretrained, in_chans=in_chans) # Pass in_chans\n",
    "\n",
    "        if del_layers:\n",
    "            del self.svt.head\n",
    "            del self.svt.patch_embeds[2]\n",
    "            del self.svt.patch_embeds[2]\n",
    "            del self.svt.blocks[2]\n",
    "            del self.svt.blocks[2]\n",
    "            del self.svt.pos_block[2]\n",
    "            del self.svt.pos_block[2]\n",
    "\n",
    "    def forward(self, x, data=None, layer=2):\n",
    "        B = x.shape[0]\n",
    "        for i, (embed, drop, blocks, pos_blk) in enumerate(zip(self.svt.patch_embeds, self.svt.pos_drops, self.svt.blocks, self.svt.pos_block)):\n",
    "\n",
    "            x, size = embed(x)\n",
    "            x = drop(x)\n",
    "            for j, blk in enumerate(blocks):\n",
    "                x = blk(x, size)\n",
    "                if j == 0:\n",
    "                    x = pos_blk(x, size)\n",
    "            if i < len(self.svt.depths) - 1:\n",
    "                x = x.reshape(B, *size, -1).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "            if i == 0:\n",
    "                x_16 = x.clone()\n",
    "            if i == layer - 1:\n",
    "                break\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extract_ml_features(self, x, data=None, layer=2):\n",
    "        res = []\n",
    "        B = x.shape[0]\n",
    "        x1 = None\n",
    "\n",
    "        for i, (embed, drop, blocks, pos_blk) in enumerate(zip(self.svt.patch_embeds, self.svt.pos_drops, self.svt.blocks, self.svt.pos_block)):\n",
    "            x, size = embed(x)\n",
    "            if i == layer - 1:\n",
    "                x1 = x.reshape(B, *size, -1).permute(0, 3, 1, 2).contiguous()\n",
    "            x = drop(x)\n",
    "            for j, blk in enumerate(blocks):\n",
    "                x = blk(x, size)\n",
    "                if j == 0:\n",
    "                    x = pos_blk(x, size)\n",
    "            if i < len(self.svt.depths) - 1:\n",
    "                x = x.reshape(B, *size, -1).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "            if i == layer - 1:\n",
    "                break\n",
    "\n",
    "        return x1, x\n",
    "\n",
    "    def compute_params(self):\n",
    "        num = 0\n",
    "\n",
    "        for i, (embed, drop, blocks, pos_blk) in enumerate(zip(self.svt.patch_embeds, self.svt.pos_drops, self.svt.blocks, self.svt.pos_block)):\n",
    "\n",
    "            for param in embed.parameters():\n",
    "                num += np.prod(param.size())\n",
    "            for param in blocks.parameters():\n",
    "                num += np.prod(param.size())\n",
    "            for param in pos_blk.parameters():\n",
    "                num += np.prod(param.size())\n",
    "            for param in drop.parameters():\n",
    "                num += np.prod(param.size())\n",
    "            if i == 1:\n",
    "                break\n",
    "        return num\n",
    "\n",
    "\n",
    "\n",
    "def bilinear_sampler(img, coords, mode=\"bilinear\"):\n",
    "    \"\"\"Wrapper for grid_sample, uses pixel coordinates\"\"\"\n",
    "    H, W = img.shape[-2:]\n",
    "    xgrid, ygrid = coords.split([1, 1], dim=-1)\n",
    "    xgrid = 2 * xgrid / (W - 1) - 1\n",
    "    ygrid = 2 * ygrid / (H - 1) - 1\n",
    "\n",
    "    grid = torch.cat([xgrid, ygrid], dim=-1)\n",
    "    img = F.grid_sample(img, grid, align_corners=True)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def coords_grid(batch, ht, wd):\n",
    "    coords = torch.meshgrid(torch.arange(ht), torch.arange(wd))\n",
    "    coords = torch.stack(coords[::-1], dim=0).float()\n",
    "    return coords[None].repeat(batch, 1, 1, 1)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        args,\n",
    "        dim,\n",
    "        max_pos_size=100,\n",
    "        heads=4,\n",
    "        dim_head=128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head**-0.5\n",
    "        inner_dim = heads * dim_head\n",
    "\n",
    "        self.to_qk = nn.Conv2d(dim, inner_dim * 2, 1, bias=False)\n",
    "\n",
    "    def forward(self, fmap):\n",
    "        heads, b, c, h, w = self.heads, *fmap.shape\n",
    "\n",
    "        q, k = self.to_qk(fmap).chunk(2, dim=1)\n",
    "\n",
    "        q, k = map(lambda t: rearrange(t, \"b (h d) x y -> b h x y d\", h=heads), (q, k))\n",
    "        q = self.scale * q\n",
    "\n",
    "        sim = einsum(\"b h x y d, b h u v d -> b h x y u v\", q, k)\n",
    "\n",
    "        sim = rearrange(sim, \"b h x y u v -> b h (x y) (u v)\")\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        return attn\n",
    "\n",
    "\n",
    "class PCBlock4_Deep_nopool_res(nn.Module):\n",
    "    def __init__(self, C_in, C_out, k_conv):\n",
    "        super().__init__()\n",
    "        self.conv_list = nn.ModuleList([nn.Conv2d(C_in, C_in, kernel, stride=1, padding=kernel // 2, groups=C_in) for kernel in k_conv])\n",
    "\n",
    "        self.ffn1 = nn.Sequential(\n",
    "            nn.Conv2d(C_in, int(1.5 * C_in), 1, padding=0),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(int(1.5 * C_in), C_in, 1, padding=0),\n",
    "        )\n",
    "        self.pw = nn.Conv2d(C_in, C_in, 1, padding=0)\n",
    "        self.ffn2 = nn.Sequential(\n",
    "            nn.Conv2d(C_in, int(1.5 * C_in), 1, padding=0),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(int(1.5 * C_in), C_out, 1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(x + self.ffn1(x))\n",
    "        for conv in self.conv_list:\n",
    "            x = F.gelu(x + conv(x))\n",
    "        x = F.gelu(x + self.pw(x))\n",
    "        x = self.ffn2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class velocity_update_block(nn.Module):\n",
    "    def __init__(self, C_in=43 + 128 + 43, C_out=43, C_hidden=64):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(C_in, C_hidden, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(C_hidden, C_hidden, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(C_hidden, C_out, 3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class SKMotionEncoder6_Deep_nopool_res(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.cor_planes = cor_planes = (args.corr_radius * 2 + 1) ** 2 * args.cost_heads_num * args.corr_levels\n",
    "        self.convc1 = PCBlock4_Deep_nopool_res(cor_planes, 128, k_conv=args.k_conv)\n",
    "        self.convc2 = PCBlock4_Deep_nopool_res(256, 192, k_conv=args.k_conv)\n",
    "\n",
    "        self.init_hidden_state = nn.Parameter(torch.randn(1, 1, 48, 1, 1))\n",
    "\n",
    "        self.convf1_ = nn.Conv2d(4, 128, 1, 1, 0) # No change needed as input is flow (2 channels) concatenated twice\n",
    "        self.convf2 = PCBlock4_Deep_nopool_res(128, 64, k_conv=args.k_conv)\n",
    "\n",
    "        self.conv = PCBlock4_Deep_nopool_res(64 + 192 + 48 * 3, 128 - 4 + 48, k_conv=args.k_conv)\n",
    "\n",
    "        self.velocity_update_block = velocity_update_block()\n",
    "\n",
    "    def sample_flo_feat(self, flow, feat):\n",
    "\n",
    "        sampled_feat = bilinear_sampler(feat.float(), flow.permute(0, 2, 3, 1))\n",
    "        return sampled_feat\n",
    "\n",
    "    def forward(self, motion_hidden_state, forward_flow, backward_flow, coords0, forward_corr, backward_corr, bs):\n",
    "\n",
    "        BN, _, H, W = forward_flow.shape\n",
    "        N = BN // bs\n",
    "\n",
    "        if motion_hidden_state is None:\n",
    "            # print(\"initialized as None\")\n",
    "            motion_hidden_state = self.init_hidden_state.repeat(bs, N, 1, H, W)\n",
    "        else:\n",
    "            # print(\"later iterations\")\n",
    "            motion_hidden_state = motion_hidden_state.reshape(bs, N, -1, H, W)\n",
    "\n",
    "        forward_loc = forward_flow + coords0\n",
    "        backward_loc = backward_flow + coords0\n",
    "\n",
    "        forward_motion_hidden_state = torch.cat([motion_hidden_state[:, 1:, ...], torch.zeros(bs, 1, 48, H, W).to(motion_hidden_state.device)], dim=1).reshape(BN, -1, H, W)\n",
    "        forward_motion_hidden_state = self.sample_flo_feat(forward_loc, forward_motion_hidden_state)\n",
    "        backward_motion_hidden_state = torch.cat([torch.zeros(bs, 1, 48, H, W).to(motion_hidden_state.device), motion_hidden_state[:, : N - 1, ...]], dim=1).reshape(BN, -1, H, W)\n",
    "        backward_motion_hidden_state = self.sample_flo_feat(backward_loc, backward_motion_hidden_state)\n",
    "\n",
    "        forward_cor = self.convc1(forward_corr)\n",
    "        backward_cor = self.convc1(backward_corr)\n",
    "        cor = F.gelu(torch.cat([forward_cor, backward_cor], dim=1))\n",
    "        cor: Tensor = self.convc2(cor)\n",
    "\n",
    "        flow = torch.cat([forward_flow, backward_flow], dim=1)\n",
    "        flo = self.convf1_(flow)\n",
    "        flo: Tensor = self.convf2(flo)\n",
    "\n",
    "        cor_flo = torch.cat([cor, flo, forward_motion_hidden_state, backward_motion_hidden_state, motion_hidden_state.reshape(BN, -1, H, W)], dim=1)\n",
    "        out = self.conv(cor_flo)\n",
    "\n",
    "        out, motion_hidden_state = torch.split(out, [124, 48], dim=1)\n",
    "\n",
    "        return torch.cat([out, flow], dim=1), motion_hidden_state\n",
    "\n",
    "\n",
    "class SKUpdateBlock6_Deep_nopoolres_AllDecoder2(nn.Module):\n",
    "    def __init__(self, args, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        args.k_conv = [1, 15]\n",
    "        args.PCUpdater_conv = [1, 7]\n",
    "\n",
    "        hidden_dim_ratio = 256 // args.feat_dim\n",
    "\n",
    "        self.encoder = SKMotionEncoder6_Deep_nopool_res(args)\n",
    "        self.gru = PCBlock4_Deep_nopool_res(128 + hidden_dim + hidden_dim + 128, 128 // hidden_dim_ratio, k_conv=args.PCUpdater_conv)\n",
    "        self.flow_head = PCBlock4_Deep_nopool_res(128 // hidden_dim_ratio, 4, k_conv=args.k_conv) # No change needed as output is flow (2 channels) concatenated twice\n",
    "\n",
    "        self.mask = nn.Sequential(\n",
    "            nn.Conv2d(128 // hidden_dim_ratio, 256 // hidden_dim_ratio, 3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(256 // hidden_dim_ratio, args.down_ratio**2 * 9 * 2, 1, padding=0)\n",
    "        )\n",
    "\n",
    "        self.aggregator = Aggregate(args=self.args, dim=128, dim_head=128, heads=1)\n",
    "\n",
    "    def forward(self, net, motion_hidden_state, inp, forward_corr, backward_corr, forward_flow, backward_flow, coords0, attention, bs):\n",
    "\n",
    "        motion_features, motion_hidden_state = self.encoder(motion_hidden_state, forward_flow, backward_flow, coords0, forward_corr, backward_corr, bs=bs)\n",
    "        motion_features_global = self.aggregator(attention, motion_features)\n",
    "        inp_cat = torch.cat([inp, motion_features, motion_features_global], dim=1)\n",
    "\n",
    "        # Attentional update\n",
    "        net = self.gru(torch.cat([net, inp_cat], dim=1))\n",
    "\n",
    "        delta_flow = self.flow_head(net)\n",
    "\n",
    "        # scale mask to balence gradients\n",
    "        mask = 100.0 * self.mask(net)\n",
    "        return net, motion_hidden_state, mask, delta_flow\n",
    "\n",
    "\n",
    "class CorrBlock:\n",
    "    def __init__(self, fmap1, fmap2, num_levels=4, radius=4):\n",
    "        self.num_levels = num_levels\n",
    "        self.radius = radius\n",
    "        self.corr_pyramid = []\n",
    "\n",
    "        # all pairs correlation\n",
    "        corr = CorrBlock.corr(fmap1, fmap2)\n",
    "\n",
    "        batch, h1, w1, dim, h2, w2 = corr.shape\n",
    "        corr = corr.reshape(batch * h1 * w1, dim, h2, w2)\n",
    "\n",
    "        self.corr_pyramid.append(corr)\n",
    "        for i in range(self.num_levels - 1):\n",
    "            corr = F.avg_pool2d(corr, 2, stride=2)\n",
    "            self.corr_pyramid.append(corr)\n",
    "\n",
    "    def __call__(self, coords):\n",
    "        r = self.radius\n",
    "        coords = coords.permute(0, 2, 3, 1)\n",
    "        batch, h1, w1, _ = coords.shape\n",
    "\n",
    "        out_pyramid = []\n",
    "        for i in range(self.num_levels):\n",
    "            corr = self.corr_pyramid[i]\n",
    "            dx = torch.linspace(-r, r, 2 * r + 1)\n",
    "            dy = torch.linspace(-r, r, 2 * r + 1)\n",
    "            delta = torch.stack(torch.meshgrid(dy, dx), dim=-1).to(coords.device)\n",
    "\n",
    "            centroid_lvl = coords.reshape(batch * h1 * w1, 1, 1, 2) / 2**i\n",
    "            delta_lvl = delta.view(1, 2 * r + 1, 2 * r + 1, 2)\n",
    "            coords_lvl = centroid_lvl + delta_lvl\n",
    "\n",
    "            corr = bilinear_sampler(corr, coords_lvl)\n",
    "            corr = corr.view(batch, h1, w1, -1)\n",
    "            out_pyramid.append(corr)\n",
    "\n",
    "        out = torch.cat(out_pyramid, dim=-1)\n",
    "        return out.permute(0, 3, 1, 2).contiguous().float()\n",
    "\n",
    "    @staticmethod\n",
    "    def corr(fmap1, fmap2):\n",
    "        batch, dim, ht, wd = fmap1.shape\n",
    "        fmap1 = fmap1.view(batch, dim, ht * wd)\n",
    "        fmap2 = fmap2.view(batch, dim, ht * wd)\n",
    "\n",
    "        corr = torch.matmul(fmap1.transpose(1, 2), fmap2)\n",
    "        corr = corr.view(batch, ht, wd, 1, ht, wd)\n",
    "        return corr / torch.sqrt(torch.tensor(dim).float())\n",
    "\n",
    "\n",
    "from torch.amp.autocast_mode import autocast\n",
    "\n",
    "class MOFNet(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.hidden_dim = hdim = self.cfg.feat_dim // 2\n",
    "        self.context_dim = cdim = self.cfg.feat_dim // 2\n",
    "\n",
    "        cfg.corr_radius = 4\n",
    "\n",
    "        # feature network, context network, and update block\n",
    "        self.cnet = twins_svt_large(pretrained=self.cfg.pretrain, in_chans=1) # in_chans=1 for grayscale\n",
    "        self.fnet = twins_svt_large(pretrained=self.cfg.pretrain, in_chans=1) # in_chans=1 for grayscale\n",
    "\n",
    "        hidden_dim_ratio = 256 // cfg.feat_dim\n",
    "\n",
    "        self.update_block = SKUpdateBlock6_Deep_nopoolres_AllDecoder2(args=self.cfg, hidden_dim=128 // hidden_dim_ratio)\n",
    "        self.att = Attention(args=self.cfg, dim=128 // hidden_dim_ratio, heads=1, max_pos_size=160, dim_head=128 // hidden_dim_ratio)\n",
    "\n",
    "    def initialize_flow(self, img, bs, down_ratio):\n",
    "        \"\"\"Flow is represented as difference between two coordinate grids flow = coords1 - coords0\"\"\"\n",
    "        N, C, H, W = img.shape\n",
    "        coords0 = coords_grid(bs, H // down_ratio, W // down_ratio).to(img.device)\n",
    "        coords1 = coords_grid(bs, H // down_ratio, W // down_ratio).to(img.device)\n",
    "\n",
    "        # optical flow computed as difference: flow = coords1 - coords0\n",
    "        return coords0, coords1\n",
    "\n",
    "    def upsample_flow(self, flow, mask):\n",
    "        \"\"\"Upsample flow field [H/8, W/8, 2] -> [H, W, 2] using convex combination\"\"\"\n",
    "        N, _, H, W = flow.shape\n",
    "        mask = mask.view(N, 1, 9, 8, 8, H, W)\n",
    "        mask = torch.softmax(mask, dim=2)\n",
    "\n",
    "        up_flow = F.unfold(8 * flow, 3, padding=1)\n",
    "        up_flow = up_flow.view(N, 2, 9, 1, 1, H, W)\n",
    "\n",
    "        up_flow = torch.sum(mask * up_flow, dim=2)\n",
    "        up_flow = up_flow.permute(0, 1, 4, 2, 5, 3)\n",
    "        return up_flow.reshape(N, 2, 8 * H, 8 * W)\n",
    "\n",
    "    def upsample_flow_4x(self, flow, mask):\n",
    "\n",
    "        N, _, H, W = flow.shape\n",
    "        mask = mask.view(N, 1, 9, 4, 4, H, W)\n",
    "        mask = torch.softmax(mask, dim=2)\n",
    "\n",
    "        up_flow = F.unfold(4 * flow, 3, padding=1)\n",
    "        up_flow = up_flow.view(N, 2, 9, 1, 1, H, W)\n",
    "\n",
    "        up_flow = torch.sum(mask * up_flow, dim=2)\n",
    "        up_flow = up_flow.permute(0, 1, 4, 2, 5, 3)\n",
    "        return up_flow.reshape(N, 2, 4 * H, 4 * W)\n",
    "\n",
    "    def upsample_flow_2x(self, flow, mask):\n",
    "\n",
    "        N, _, H, W = flow.shape\n",
    "        mask = mask.view(N, 1, 9, 2, 2, H, W)\n",
    "        mask = torch.softmax(mask, dim=2)\n",
    "\n",
    "        up_flow = F.unfold(2 * flow, 3, padding=1)\n",
    "        up_flow = up_flow.view(N, 2, 9, 1, 1, H, W)\n",
    "\n",
    "        up_flow = torch.sum(mask * up_flow, dim=2)\n",
    "        up_flow = up_flow.permute(0, 1, 4, 2, 5, 3)\n",
    "        return up_flow.reshape(N, 2, 2 * H, 2 * W)\n",
    "\n",
    "    def forward(self, images, data={}, flow_init=None):\n",
    "\n",
    "        down_ratio = self.cfg.down_ratio\n",
    "\n",
    "        B, N, _, H, W = images.shape\n",
    "\n",
    "        images = 2 * (images / 255.0) - 1.0\n",
    "\n",
    "        hdim = self.hidden_dim\n",
    "        cdim = self.context_dim\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=self.cfg.mixed_precision):\n",
    "            fmaps = self.fnet(images.reshape(B * N, 1, H, W)).reshape(B, N, -1, H // down_ratio, W // down_ratio) # Changed 3 to 1\n",
    "        fmaps = fmaps.float()\n",
    "\n",
    "        forward_corr_fn = CorrBlock(\n",
    "            fmaps[:, 1 : N - 1, ...].reshape(B * (N - 2), -1, H // down_ratio, W // down_ratio),\n",
    "            fmaps[:, 2:N, ...].reshape(B * (N - 2), -1, H // down_ratio, W // down_ratio),\n",
    "            num_levels=self.cfg.corr_levels,\n",
    "            radius=self.cfg.corr_radius,\n",
    "        )\n",
    "        backward_corr_fn = CorrBlock(\n",
    "            fmaps[:, 1 : N - 1, ...].reshape(B * (N - 2), -1, H // down_ratio, W // down_ratio),\n",
    "            fmaps[:, 0 : N - 2, ...].reshape(B * (N - 2), -1, H // down_ratio, W // down_ratio),\n",
    "            num_levels=self.cfg.corr_levels,\n",
    "            radius=self.cfg.corr_radius,\n",
    "        )\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=self.cfg.mixed_precision):\n",
    "            cnet = self.cnet(images[:, 1 : N - 1, ...].reshape(B * (N - 2), 1, H, W)) # Changed 3 to 1\n",
    "            net, inp = torch.split(cnet, [hdim, cdim], dim=1)\n",
    "            net = torch.tanh(net)\n",
    "            inp = torch.relu(inp)\n",
    "            attention = self.att(inp)\n",
    "\n",
    "        forward_coords1, forward_coords0 = self.initialize_flow(images[:, 0, ...], bs=B * (N - 2), down_ratio=down_ratio)\n",
    "        backward_coords1, backward_coords0 = self.initialize_flow(images[:, 0, ...], bs=B * (N - 2), down_ratio=down_ratio)\n",
    "\n",
    "        flow_predictions = []  # forward flows followed by backward flows\n",
    "\n",
    "        motion_hidden_state = None\n",
    "\n",
    "        for itr in range(self.cfg.decoder_depth):\n",
    "\n",
    "            forward_coords1 = forward_coords1.detach()\n",
    "            backward_coords1 = backward_coords1.detach()\n",
    "\n",
    "            forward_corr = forward_corr_fn(forward_coords1)\n",
    "            backward_corr = backward_corr_fn(backward_coords1)\n",
    "\n",
    "            forward_flow = forward_coords1 - forward_coords0\n",
    "            backward_flow = backward_coords1 - backward_coords0\n",
    "\n",
    "            with autocast(device_type=\"cuda\", enabled=self.cfg.mixed_precision):\n",
    "                net, motion_hidden_state, up_mask, delta_flow = self.update_block(\n",
    "                    net, motion_hidden_state, inp, forward_corr, backward_corr, forward_flow, backward_flow, forward_coords0, attention, bs=B\n",
    "                )\n",
    "\n",
    "            forward_up_mask, backward_up_mask = torch.split(up_mask, [down_ratio**2 * 9, down_ratio**2 * 9], dim=1)\n",
    "\n",
    "            forward_coords1 = forward_coords1 + delta_flow[:, 0:2, ...]\n",
    "            backward_coords1 = backward_coords1 + delta_flow[:, 2:4, ...]\n",
    "\n",
    "            # upsample predictions\n",
    "            if down_ratio == 4:\n",
    "                forward_flow_up = self.upsample_flow_4x(forward_coords1 - forward_coords0, forward_up_mask)\n",
    "                backward_flow_up = self.upsample_flow_4x(backward_coords1 - backward_coords0, backward_up_mask)\n",
    "            elif down_ratio == 2:\n",
    "                forward_flow_up = self.upsample_flow_2x(forward_coords1 - forward_coords0, forward_up_mask)\n",
    "                backward_flow_up = self.upsample_flow_2x(backward_coords1 - backward_coords0, backward_up_mask)\n",
    "            else:\n",
    "                forward_flow_up = self.upsample_flow(forward_coords1 - forward_coords0, forward_up_mask)\n",
    "                backward_flow_up = self.upsample_flow(backward_coords1 - backward_coords0, backward_up_mask)\n",
    "\n",
    "            flow_predictions.append(torch.cat([forward_flow_up.reshape(B, N - 2, 2, H, W), backward_flow_up.reshape(B, N - 2, 2, H, W)], dim=1))\n",
    "\n",
    "        if self.training:\n",
    "            return flow_predictions\n",
    "        else:\n",
    "            return flow_predictions[-1], flow_predictions[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.feat_dim = 256\n",
    "        self.corr_levels = 4\n",
    "        self.corr_radius = 4\n",
    "        self.corr_fn = \"default\"\n",
    "        self.mixed_precision = True\n",
    "        self.decoder_depth = 32\n",
    "        self.down_ratio = 8\n",
    "        self.pretrain = True  # for using pretrained model or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()  # Create an instance of your configuration\n",
    "model = MOFNet(cfg)  # Create the MOFNet model\n",
    "model = model.cuda()  # move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example input (replace with your actual data)\n",
    "images = torch.randn(1, 3, 1, 256, 256).cuda()  # Batch size 2, 3 frames, 1 channel, 256x256 # Changed 3 to 1\n",
    "\n",
    "flow_predictions = model(images)\n",
    "\n",
    "print(len(flow_predictions))  # Print the shape of the output\n",
    "print(flow_predictions[-1].shape)  # Print the shape of the output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
